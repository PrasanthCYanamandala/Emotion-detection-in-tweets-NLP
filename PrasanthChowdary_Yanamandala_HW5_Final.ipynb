{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPZDipUy68SgIeSKiZtYlgE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["setting up the environment"],"metadata":{"id":"IM_sOrobYmkd"}},{"cell_type":"code","source":["if 'google.colab' in str(get_ipython()):\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","\n","    basepath = '/content/drive/MyDrive'\n","\n","else:\n","    basepath = '/home/harpreet/Insync/google_drive_shaannoor/data'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6aKbQQ5YpUF","executionInfo":{"status":"ok","timestamp":1730509371695,"user_tz":300,"elapsed":20313,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}},"outputId":"2d695a44-77d0-47fb-c9ba-660c84cb14a2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["Load Libraries"],"metadata":{"id":"SZ5Xm-2UZL0d"}},{"cell_type":"code","source":["pip install datasets evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L83wdSCOZcLo","executionInfo":{"status":"ok","timestamp":1730509451578,"user_tz":300,"elapsed":5074,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}},"outputId":"7d7bec5c-6485-4f1d-abab-6a223b7b246e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n","Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["# Importing PyTorch library for tensor computations and neural network modules\n","import torch\n","import torch.nn as nn\n","\n","# For working with textual data vocabularies and for displaying model summaries\n","\n","# General-purpose Python libraries for random number generation and numerical operations\n","import random\n","import numpy as np\n","\n","# Utilities for efficient serialization/deserialization of Python objects and for element tallying\n","import joblib\n","from collections import Counter\n","\n","# For creating lightweight attribute classes and for partial function application\n","from functools import partial\n","\n","# For filesystem path handling, generating and displaying confusion matrices, and date-time manipulations\n","from pathlib import Path\n","from sklearn.metrics import confusion_matrix\n","from datetime import datetime\n","\n","# For plotting and visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# %matplotlib inline\n","\n","### NEW ##########################\n","# imports from Huggingface ecosystem\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","from transformers import PreTrainedModel, PretrainedConfig\n","from transformers import TrainingArguments, Trainer\n","from datasets import Dataset\n","import evaluate\n","\n","# wandb library\n","import wandb"],"metadata":{"id":"gkpSAFKcZNoK","executionInfo":{"status":"ok","timestamp":1730509467014,"user_tz":300,"elapsed":10326,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score"],"metadata":{"id":"atIOLduZib3s","executionInfo":{"status":"ok","timestamp":1730511764967,"user_tz":300,"elapsed":150,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# CHANGE FOLDERS TO WHERE YOU WANT TO SAVE DATA AND MODELS\n","base_folder = Path(basepath)\n","data_folder = base_folder/'datasets/HW6/Datafolder'"],"metadata":{"id":"RTioyyEEaJc_","executionInfo":{"status":"ok","timestamp":1730509580347,"user_tz":300,"elapsed":180,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Load dataset"],"metadata":{"id":"iEEwZjxUaKuq"}},{"cell_type":"code","source":["import pandas as pd\n","train_df = pd.read_csv(data_folder/'train.csv')\n","test_df = pd.read_csv(data_folder/'test.csv')\n","sample_submission = pd.read_csv(data_folder/'sample_submission.csv')"],"metadata":{"id":"fnvYEyzXaMPk","executionInfo":{"status":"ok","timestamp":1730514519546,"user_tz":300,"elapsed":136,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}}},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":["Data cleaning"],"metadata":{"id":"5SZivS0XeJn8"}},{"cell_type":"code","source":["pip install emoji"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3Rzb_M8eIkr","executionInfo":{"status":"ok","timestamp":1730510638786,"user_tz":300,"elapsed":3354,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}},"outputId":"4cf30f5e-d829-40d0-dedc-9150cdfa3474"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emoji\n","  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n","Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/586.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m583.7/586.9 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: emoji\n","Successfully installed emoji-2.14.0\n"]}]},{"cell_type":"code","source":["# Define a function to clean the tweets\n","import re\n","import emoji\n","\n","def clean_tweet(tweet):\n","    tweet = tweet.lower()\n","    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n","    tweet = re.sub(r'@\\w+', '', tweet)\n","    tweet = re.sub(r'#', '', tweet)\n","    tweet = re.sub(r'\\d+', '', tweet)\n","    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n","    tweet = emoji.demojize(tweet, delimiters=(\" \", \" \"))\n","    return tweet\n","\n","train_df['Cleaned_Tweet'] = train_df['Tweet'].apply(clean_tweet)\n","test_df['Cleaned_Tweet'] = test_df['Tweet'].apply(clean_tweet)"],"metadata":{"id":"fjnJqNBNeMch","executionInfo":{"status":"ok","timestamp":1730514523291,"user_tz":300,"elapsed":1487,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}}},"execution_count":125,"outputs":[]},{"cell_type":"code","source":["# Preprocess labels in test_df to be integers (all 0s)\n","for emotion in ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']:\n","    test_df[emotion] = 0"],"metadata":{"id":"HQZFI5GDeVmy","executionInfo":{"status":"ok","timestamp":1730514525276,"user_tz":300,"elapsed":177,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}}},"execution_count":126,"outputs":[]},{"cell_type":"code","source":["# Keep only the 'Tweet' (as 'text') and 'label' columns\n","train_df = train_df[['ID', 'Cleaned_Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']].rename(columns={'Cleaned_Tweet': 'Tweet'})\n","test_df = test_df[['ID', 'Cleaned_Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']].rename(columns={'Cleaned_Tweet': 'Tweet'})\n"],"metadata":{"id":"2KeP_DioeY1Y","executionInfo":{"status":"ok","timestamp":1730514527195,"user_tz":300,"elapsed":227,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}}},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":["Tokenizer"],"metadata":{"id":"63j19LW8ikOo"}},{"cell_type":"code","source":["# Define Custom Tokenizer with updated max_length\n","class CustomTokenizer:\n","    def __init__(self, max_length=130):  # Updated to 130\n","        self.max_length = max_length\n","        self.char2id = {\"[PAD]\": 0, \"[UNK]\": 1}\n","        self.id2char = {0: \"[PAD]\", 1: \"[UNK]\"}\n","        self.vocab_size = 2\n","\n","    def build_vocab(self, texts):\n","        for text in texts:\n","            for char in text:\n","                if char not in self.char2id:\n","                    self.char2id[char] = self.vocab_size\n","                    self.id2char[self.vocab_size] = char\n","                    self.vocab_size += 1\n","\n","    def tokenize(self, text):\n","        tokens = [self.char2id.get(char, self.char2id[\"[UNK]\"]) for char in text]\n","        if len(tokens) > self.max_length:\n","            tokens = tokens[:self.max_length]\n","        else:\n","            tokens += [self.char2id[\"[PAD]\"]] * (self.max_length - len(tokens))\n","        return tokens"],"metadata":{"id":"EP-OmhWdilUd","executionInfo":{"status":"ok","timestamp":1730514551478,"user_tz":300,"elapsed":164,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}}},"execution_count":128,"outputs":[]},{"cell_type":"code","source":["# Initialize tokenizer and build vocabulary\n","tokenizer = CustomTokenizer(max_length=130)\n","all_texts = pd.concat([train_df['Tweet'], test_df['Tweet']], ignore_index=True)\n","tokenizer.build_vocab(all_texts)"],"metadata":{"id":"bHScHzPqio7Q","executionInfo":{"status":"ok","timestamp":1730514563399,"user_tz":300,"elapsed":199,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}}},"execution_count":129,"outputs":[]},{"cell_type":"markdown","source":["Huggingface dataset"],"metadata":{"id":"XJ7zeF5hityZ"}},{"cell_type":"code","source":["# Convert train and test data to Hugging Face Datasets with float labels\n","def preprocess_data(df, is_train=True):\n","    texts = df['Tweet'].tolist()\n","    input_ids = [tokenizer.tokenize(text) for text in texts]\n","    if is_train:\n","        labels = df.iloc[:, 2:].values.astype(float).tolist()  # Convert labels to float\n","        return {\"input_ids\": input_ids, \"labels\": labels}\n","    else:\n","        return {\"input_ids\": input_ids, \"ID\": df['ID'].tolist()}"],"metadata":{"id":"GvlQLC_3ir_k","executionInfo":{"status":"ok","timestamp":1730514583643,"user_tz":300,"elapsed":207,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}}},"execution_count":130,"outputs":[]},{"cell_type":"code","source":["train_dataset = Dataset.from_dict(preprocess_data(train_df, is_train=True))\n","test_dataset = Dataset.from_dict(preprocess_data(test_df, is_train=False))"],"metadata":{"id":"o4WAaElti0j-","executionInfo":{"status":"ok","timestamp":1730514594342,"user_tz":300,"elapsed":889,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}}},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":["Model"],"metadata":{"id":"7jAMof6RkyP7"}},{"cell_type":"code","source":["# Define the Model with explicit loss function handling\n","class SimpleClassifier(nn.Module):\n","    def __init__(self, input_dim=130, num_labels=train_df.shape[1] - 2):\n","        super(SimpleClassifier, self).__init__()\n","        self.embedding = nn.Embedding(256, 128)\n","        self.fc1 = nn.Linear(130 * 128, 64)\n","        self.fc2 = nn.Linear(64, num_labels)\n","        self.loss_fn = nn.BCEWithLogitsLoss()  # Use BCEWithLogitsLoss for multi-label classification\n","\n","    def forward(self, input_ids, labels=None):\n","        x = self.embedding(input_ids)\n","        x = x.view(x.size(0), -1)\n","        x = torch.relu(self.fc1(x))\n","        logits = self.fc2(x)  # Outputs raw logits without sigmoid\n","\n","        # Compute loss if labels are provided (for training)\n","        if labels is not None:\n","            loss = self.loss_fn(logits, labels)\n","            return {\"loss\": loss, \"logits\": logits}\n","        else:\n","            return {\"logits\": logits}  # Only logits for inference"],"metadata":{"id":"mlh2goN6kzL7","executionInfo":{"status":"ok","timestamp":1730514613566,"user_tz":300,"elapsed":152,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}}},"execution_count":132,"outputs":[]},{"cell_type":"code","source":["# Instantiate the model\n","model = SimpleClassifier(input_dim=130, num_labels=train_df.shape[1] - 2)"],"metadata":{"id":"xs3RmGQ3k5Eh","executionInfo":{"status":"ok","timestamp":1730514634935,"user_tz":300,"elapsed":247,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}}},"execution_count":133,"outputs":[]},{"cell_type":"markdown","source":["Compute metrics"],"metadata":{"id":"jTEXRRGqk7ZP"}},{"cell_type":"code","source":["# Define metric function for evaluation\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = (pred.predictions > 0.5).astype(int)\n","    accuracy = accuracy_score(labels, preds)\n","    f1 = f1_score(labels, preds, average='macro')\n","    return {'accuracy': accuracy, 'f1': f1}"],"metadata":{"id":"VUV4lqsck82Q","executionInfo":{"status":"ok","timestamp":1730514646405,"user_tz":300,"elapsed":160,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}}},"execution_count":134,"outputs":[]},{"cell_type":"markdown","source":["Trainer"],"metadata":{"id":"_vJFtdJDk_pG"}},{"cell_type":"code","source":["# Set up TrainingArguments and Trainer\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    evaluation_strategy=\"epoch\",\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    num_train_epochs=5,\n","    save_strategy=\"epoch\",\n",")\n","\n","# Split train_dataset into training and validation sets using train_test_split directly\n","split_dataset = train_dataset.train_test_split(test_size=0.1)  # 10% for validation\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=split_dataset['train'],\n","    eval_dataset=split_dataset['test'],  # Use validation dataset for evaluation\n","    tokenizer=None,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yNb0UY_KlDpn","executionInfo":{"status":"ok","timestamp":1730514695333,"user_tz":300,"elapsed":206,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}},"outputId":"cb73d178-f7a9-49e9-e02c-bba0c7005f44"},"execution_count":135,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Train the model\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":268},"id":"PBwV_6FulHMP","executionInfo":{"status":"ok","timestamp":1730514779308,"user_tz":300,"elapsed":35676,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}},"outputId":"9bcef2df-e903-4b84-d066-c12e54434879"},"execution_count":137,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='8690' max='8690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [8690/8690 00:35, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.382600</td>\n","      <td>0.494639</td>\n","      <td>0.051746</td>\n","      <td>0.088273</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.344500</td>\n","      <td>0.509640</td>\n","      <td>0.055627</td>\n","      <td>0.127829</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.310300</td>\n","      <td>0.512032</td>\n","      <td>0.056921</td>\n","      <td>0.133307</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.297600</td>\n","      <td>0.520010</td>\n","      <td>0.062096</td>\n","      <td>0.140951</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.283500</td>\n","      <td>0.522727</td>\n","      <td>0.056921</td>\n","      <td>0.136143</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=8690, training_loss=0.3176752255069921, metrics={'train_runtime': 35.1812, 'train_samples_per_second': 987.887, 'train_steps_per_second': 247.007, 'total_flos': 0.0, 'train_loss': 0.3176752255069921, 'epoch': 5.0})"]},"metadata":{},"execution_count":137}]},{"cell_type":"markdown","source":["Prediction on test and submission"],"metadata":{"id":"xCLtW2bRsMcf"}},{"cell_type":"code","source":["# Predict on the test set\n","predictions = trainer.predict(test_dataset)\n","test_preds = (predictions.predictions > 0.5).astype(int)\n","\n","# Prepare submission file using sample_submission format\n","submission = sample_submission[['ID']].copy()\n","submission[['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']] = pd.DataFrame(test_preds, index=submission.index)\n","\n","# Save the submission file\n","submission.to_csv('submission_B.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"iVLFl8VHsPYo","executionInfo":{"status":"ok","timestamp":1730514798642,"user_tz":300,"elapsed":3331,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}},"outputId":"999c7bcf-9f1c-43d5-d760-f239e6a92a3e"},"execution_count":139,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}]},{"cell_type":"code","source":["from google.colab import files\n","# Download the saved submission file\n","files.download(\"submission_B.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"SH2gMFTMqqNB","executionInfo":{"status":"ok","timestamp":1730514800366,"user_tz":300,"elapsed":158,"user":{"displayName":"Prasanth chowdary","userId":"11787109958518731185"}},"outputId":"359e748e-ae7f-4b1b-ddb1-01046c538895"},"execution_count":140,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_83d12771-e3d6-4c77-95ef-d71a176a0809\", \"submission_B.csv\", 107633)"]},"metadata":{}}]}]}